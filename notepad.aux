\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand{\transparent@use}[1]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{kingma2017adammethodstochasticoptimization}
\citation{loshchilov2019decoupledweightdecayregularization}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Understanding Adam}{7}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap: Understanding Adam}{{2}{7}{Understanding Adam}{chapter.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Adam Optimizer \leavevmode {\color  {blue}(Bias Correction Experimental Focus)}\relax }}{7}{algorithm.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:adam}{{1}{7}{Adam Optimizer \textcolor {blue}{(Bias Correction Experimental Focus)}\relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Why is AdamW the Top Dog?}{7}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Preliminaries and Related Work}{8}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}EAdam Paper probably}{8}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Relation to signSGD}{8}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Why transformers need Adam: hessian and class imbalance}{8}{subsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}The Secret Sauce paper}{8}{subsection.2.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Some Experiments/Ablations (Mostly an experiment graveyard sadly)}{8}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}}{9}{subsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Nesting the Moving averages}{9}{subsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Adam2SGD and $\varepsilon $ schedule}{9}{subsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}How EMA smooths the update signal}{9}{subsection.2.3.4}\protected@file@percent }
\citation{yuan2020eadamoptimizerepsilonimpact}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}BackPACK}{10}{subsection.2.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}The $\varepsilon $ Hyperparameter}{10}{subsection.2.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Bias Correction in Adam: A Detailed Analysis}{11}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap: bias correction}{{3}{11}{Bias Correction in Adam: A Detailed Analysis}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Bias Correction: An Unnecessary Evil? }{11}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}A Discussion of the "Proof" for Bias Correction}{11}{subsection.3.1.1}\protected@file@percent }
\citation{Idelbayev18a}
\citation{huang2018denselyconnectedconvolutionalnetworks}
\citation{Karpathy2022}
\citation{swiGLU}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Experimental Outline}{12}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Is Bias Correction just Hidden Learning rate scheduling?}{15}{section.3.3}\protected@file@percent }
\newlabel{eq: bias factor}{{3.1}{15}{Is Bias Correction just Hidden Learning rate scheduling?}{equation.3.3.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces \textit  {ZI} denotes Zero init, \textit  {BC} denotes Bias Correction. Not doing ZI means we initialize $m$ and $v$ at $g_0$ and $g_0^2$ respectively. Default for AdamW is ZI and BC. Performing bias correction is not as important as initialization in Adam. Averaged results over 4 random seeds  HPs $\text  {lr:}0.008, \beta _1:0.95, \beta _2: 0.95, wd:0.1$\relax }}{17}{table.caption.4}\protected@file@percent }
\citation{NGdescent}
\citation{moonshot}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Beyond Adam: Is Muon the future?}{19}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{bernstein2024modulardualitydeeplearning}
\citation{bernstein2024oldoptimizernewnorm}
\citation{bernstein2025deriving}
\citation{jordan2024muon}
\citation{pethick2025trainingdeeplearningmodels}
\citation{bernstein2024modulardualitydeeplearning}
\citation{gupta2018shampoopreconditionedstochastictensor}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Related Work}{20}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Understanding Muon}{20}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Metrized Deep Learning}{20}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Dualized Gradients and Metrized Deep Learning}{20}{subsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Proof of Orthogonal Update step}{20}{subsection.4.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Muon's Relationship to Shampoo}{20}{subsection.4.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}The Newton Schulz Algorithm}{20}{subsection.4.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Ablations on Simple Problems}{20}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Linear Regression}{21}{subsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}A Quadratic Problem}{22}{subsection.4.3.2}\protected@file@percent }
\newlabel{eq: simple quadratic}{{4.1}{22}{A Quadratic Problem}{equation.4.3.1}{}}
\newlabel{eq: matquadratic}{{4.2}{22}{A Quadratic Problem}{equation.4.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Logistic Regression}{23}{subsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Small MLPs}{23}{subsection.4.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}SVD structural analysis}{23}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}CIFAR-10 ResNet}{23}{subsection.4.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Ablation varying number of Newton Schulz steps performed and comparing with an exact orthogonalization computed using the SVD of the matrix. Each data point on the x-axis corresponds to an averaged final test accuracy over 20 random seeds with error bars representing a 95\% confidence interval. \relax }}{24}{figure.caption.6}\protected@file@percent }
\newlabel{fig:muon_cifar_NS}{{4.1}{24}{Ablation varying number of Newton Schulz steps performed and comparing with an exact orthogonalization computed using the SVD of the matrix. Each data point on the x-axis corresponds to an averaged final test accuracy over 20 random seeds with error bars representing a 95\% confidence interval. \relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Muon on nanoGPT and plainLM}{24}{section.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Experimental }{24}{section.4.6}\protected@file@percent }
\bibstyle{alpha}
\bibdata{bibliography}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Discussion and Future Work}{25}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{NGdescent}{Ama98}
\bibcite{bernstein2025deriving}{Ber25}
\bibcite{bernstein2024modulardualitydeeplearning}{BN24a}
\bibcite{bernstein2024oldoptimizernewnorm}{BN24b}
\bibcite{gupta2018shampoopreconditionedstochastictensor}{GKS18}
\bibcite{huang2018denselyconnectedconvolutionalnetworks}{HLvdMW18}
\bibcite{Idelbayev18a}{Ide}
\bibcite{jordan2024muon}{JJB{$^{+}$}24}
\bibcite{Karpathy2022}{Kar22}
\bibcite{kingma2017adammethodstochasticoptimization}{KB17}
\bibcite{loshchilov2019decoupledweightdecayregularization}{LH19}
\bibcite{moonshot}{LSY{$^{+}$}25}
\bibcite{pethick2025trainingdeeplearningmodels}{PXA{$^{+}$}25}
\bibcite{swiGLU}{Sha20}
\bibcite{yuan2020eadamoptimizerepsilonimpact}{YG20}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}How Exponential Moving Averages Smooth the Update Signal}{29}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\gdef \@abspage@last{29}
