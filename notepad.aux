\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand{\transparent@use}[1]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{kingma2017adammethodstochasticoptimization}
\citation{loshchilov2019decoupledweightdecayregularization}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Understanding Adam}{7}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap: Understanding Adam}{{2}{7}{Understanding Adam}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}A (very) Brief History of Optimization}{7}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Why is AdamW the Top Dog?}{7}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Preliminaries and Related Work}{7}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}EAdam Paper probably}{8}{subsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Relation to signSGD}{8}{subsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Why transformers need Adam: hessian and class imbalance}{8}{subsection.2.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}The Secret Sauce paper}{8}{subsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Some Experiments/Ablations (Mostly an experiment graveyard sadly)}{8}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}}{8}{subsection.2.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Nesting the Moving averages}{8}{subsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Adam2SGD and $\varepsilon $ schedule}{9}{subsection.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}How EMA smooths the update signal}{9}{subsection.2.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.5}BackPACK}{9}{subsection.2.4.5}\protected@file@percent }
\citation{yuan2020eadamoptimizerepsilonimpact}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.6}The $\varepsilon $ Hyperparameter}{10}{subsection.2.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Bias Correction in Adam: A Detailed Analysis}{11}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap: bias correction}{{3}{11}{Bias Correction in Adam: A Detailed Analysis}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Bias Correction: An Unnecessary Evil? }{11}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}A Discussion of the "Proof" for Bias Correction}{11}{subsection.3.1.1}\protected@file@percent }
\citation{Idelbayev18a}
\citation{huang2018denselyconnectedconvolutionalnetworks}
\citation{Karpathy2022}
\citation{swiGLU}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Experimental Outline}{12}{subsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Is Bias Correction just Hidden Learning rate scheduling?}{15}{section.3.2}\protected@file@percent }
\newlabel{eq: bias factor}{{3.1}{15}{Is Bias Correction just Hidden Learning rate scheduling?}{equation.3.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces \textit  {ZI} denotes Zero init, \textit  {BC} denotes Bias Correction. Not doing ZI means we initialize $m$ and $v$ at $g_0$ and $g_0^2$ respectively. Default for AdamW is ZI and BC. Performing bias correction is not as important as initialization in Adam. Averaged results over 4 random seeds  HPs $\text  {lr:}0.008, \beta _1:0.95, \beta _2: 0.95, wd:0.1$\relax }}{17}{table.caption.4}\protected@file@percent }
\citation{NGdescent}
\citation{moonshot}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Beyond Adam: Is Muon the future?}{19}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{bernstein2024modulardualitydeeplearning}
\citation{bernstein2024oldoptimizernewnorm}
\citation{bernstein2025deriving}
\citation{jordan2024muon}
\citation{pethick2025trainingdeeplearningmodels}
\citation{bernstein2024modulardualitydeeplearning}
\citation{gupta2018shampoopreconditionedstochastictensor}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Related Work}{20}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Understanding Muon}{20}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Metrized Deep Learning}{20}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Dualized Gradients and Metrized Deep Learning}{20}{subsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Proof of Orthogonal Update step}{20}{subsection.4.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Muon's Relationship to Shampoo}{20}{subsection.4.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}The Newton Schulz Algorithm}{20}{subsection.4.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Ablations on Simple Problems}{20}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Linear Regression}{21}{subsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}A Quadratic Problem}{22}{subsection.4.3.2}\protected@file@percent }
\newlabel{eq: simple quadratic}{{4.1}{22}{A Quadratic Problem}{equation.4.3.1}{}}
\newlabel{eq: matquadratic}{{4.2}{22}{A Quadratic Problem}{equation.4.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Logistic Regression}{23}{subsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Small MLPs}{23}{subsection.4.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}SVD structural analysis}{23}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}CIFAR-10 ResNet}{23}{subsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Muon on nanoGPT and plainLM}{24}{section.4.5}\protected@file@percent }
\bibstyle{plain}
\bibdata{bibliography}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Discussion and Future Work}{25}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{NGdescent}{1}
\bibcite{bernstein2025deriving}{2}
\bibcite{bernstein2024modulardualitydeeplearning}{3}
\bibcite{bernstein2024oldoptimizernewnorm}{4}
\bibcite{gupta2018shampoopreconditionedstochastictensor}{5}
\bibcite{huang2018denselyconnectedconvolutionalnetworks}{6}
\bibcite{Idelbayev18a}{7}
\bibcite{jordan2024muon}{8}
\bibcite{Karpathy2022}{9}
\bibcite{kingma2017adammethodstochasticoptimization}{10}
\bibcite{moonshot}{11}
\bibcite{loshchilov2019decoupledweightdecayregularization}{12}
\bibcite{pethick2025trainingdeeplearningmodels}{13}
\bibcite{swiGLU}{14}
\bibcite{yuan2020eadamoptimizerepsilonimpact}{15}
\gdef \@abspage@last{28}
