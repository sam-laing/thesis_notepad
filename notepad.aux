\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand{\transparent@use}[1]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Abstract}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Introduction}{7}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{kingma2017adammethodstochasticoptimization}
\citation{loshchilov2019decoupledweightdecayregularization}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Understanding Adam}{9}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap: Understanding Adam}{{3}{9}{Understanding Adam}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Preliminaries and Related Work}{9}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Some Experiments/Ablations (Mostly an experiment graveyard sadly)}{10}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Nesting the Moving averages}{10}{subsection.3.2.1}\protected@file@percent }
\citation{yuan2020eadamoptimizerepsilonimpact}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Adam2SGD and $\varepsilon $ schedule}{11}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}How EMA smooths the update signal}{11}{subsection.3.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}BackPACK}{11}{subsection.3.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}The $\varepsilon $ Hyperparameter}{11}{subsection.3.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces  Sensitivity of AdamW's final validation performance (100M held-out tokens) to $\epsilon $ placement on ~162M parameter transformer. Compares $\epsilon $ inside square root ($\frac  {m_t}{\sqrt  {v_t+\varepsilon } }$) versus outside ($\frac  {m_t}{\sqrt  {v_t} + \varepsilon }$). Averaged across 3 random seeds. The minima for each configuration are annotated. All other hyperpararameters are   We observe that placing $\varepsilon $ inside square root is more sensitive to the choice of $\varepsilon $ but optimal performance is still similar. \relax }}{12}{figure.caption.7}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:adamw_epsilon}{{3.1}{12}{Sensitivity of AdamW's final validation performance (100M held-out tokens) to $\epsilon $ placement on ~162M parameter transformer. Compares $\epsilon $ inside square root ($\frac {m_t}{\sqrt {v_t+\varepsilon } }$) versus outside ($\frac {m_t}{\sqrt {v_t} + \varepsilon }$). Averaged across 3 random seeds. The minima for each configuration are annotated. All other hyperpararameters are \\ We observe that placing $\varepsilon $ inside square root is more sensitive to the choice of $\varepsilon $ but optimal performance is still similar. \relax }{figure.caption.7}{}}
\citation{Goodfellow-et-al-2016}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Bias Correction in Adam: A Detailed Analysis}{13}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap: bias correction}{{4}{13}{Bias Correction in Adam: A Detailed Analysis}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Bias Correction: An Unnecessary Evil? }{13}{section.4.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces AdamW Optimizer \leavevmode {\color  {blue}(Bias Correction Experimental Focus)}\relax }}{13}{algorithm.1}\protected@file@percent }
\newlabel{alg:adam}{{1}{13}{AdamW Optimizer \textcolor {blue}{(Bias Correction Experimental Focus)}\relax }{algorithm.1}{}}
\citation{Goodfellow-et-al-2016}
\citation{kingma2017adammethodstochasticoptimization}
\citation{Idelbayev18a}
\citation{huang2018denselyconnectedconvolutionalnetworks}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}A Discussion of the "Proof" for Bias Correction}{14}{subsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Experimental Outline}{14}{section.4.2}\protected@file@percent }
\citation{Idelbayev18a}
\citation{Karpathy2022}
\citation{swiGLU}
\newlabel{tab: vision}{{\caption@xref {tab: vision}{ on input line 598}}{15}{Experimental Outline}{table.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Overview of datasets and model architectures used in experiments.\relax }}{15}{table.caption.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Is Bias Correction just Hidden Learning rate scheduling?}{17}{section.4.3}\protected@file@percent }
\newlabel{eq: bias factor}{{4.1}{17}{Is Bias Correction just Hidden Learning rate scheduling?}{equation.4.3.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces \textit  {ZI} denotes Zero init, \textit  {BC} denotes Bias Correction. Not doing ZI means we initialize $m$ and $v$ at $g_0$ and $g_0^2$ respectively. Default for AdamW is ZI and BC. Performing bias correction is not as important as initialization in Adam. Averaged results over 4 random seeds  HPs $\text  {lr:}0.008, \beta _1:0.95, \beta _2: 0.95, wd:0.1$\relax }}{20}{table.caption.10}\protected@file@percent }
\citation{NGdescent}
\citation{moonshot}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Beyond Adam: Is Muon the future?}{21}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{bernstein2024modulardualitydeeplearning}
\citation{bernstein2024oldoptimizernewnorm}
\citation{bernstein2025deriving}
\citation{jordan2024muon}
\citation{pethick2025trainingdeeplearningmodels}
\citation{bernstein2024modulardualitydeeplearning}
\citation{gupta2018shampoopreconditionedstochastictensor}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Related Work}{22}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Understanding Muon}{22}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Metrized Deep Learning}{22}{subsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Dualized Gradients and Metrized Deep Learning}{22}{subsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Proof of Orthogonal Update step}{22}{subsection.5.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}Muon's Relationship to Shampoo}{22}{subsection.5.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.5}The Newton Schulz Algorithm}{22}{subsection.5.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Ablations on Simple Problems}{22}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Linear Regression}{23}{subsection.5.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}A Quadratic Problem}{24}{subsection.5.3.2}\protected@file@percent }
\newlabel{eq: simple quadratic}{{5.1}{24}{A Quadratic Problem}{equation.5.3.1}{}}
\newlabel{eq: matquadratic}{{5.2}{24}{A Quadratic Problem}{equation.5.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Logistic Regression}{24}{subsection.5.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.4}Small MLPs}{25}{subsection.5.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}SVD structural analysis}{25}{section.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}CIFAR-10 ResNet}{25}{subsection.5.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Ablation varying number of Newton Schulz steps performed and comparison with performance with an exact orthogonalization computed using the SVD of the matrix. Each data point on the x-axis corresponds to an averaged final test accuracy over 20 random seeds with error bars representing a 95\% confidence interval. Other hyperparameters are as in record speedrum configuration. \relax }}{26}{figure.caption.12}\protected@file@percent }
\newlabel{fig:muon_cifar_NS}{{5.1}{26}{Ablation varying number of Newton Schulz steps performed and comparison with performance with an exact orthogonalization computed using the SVD of the matrix. Each data point on the x-axis corresponds to an averaged final test accuracy over 20 random seeds with error bars representing a 95\% confidence interval. Other hyperparameters are as in record speedrum configuration. \relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Muon on nanoGPT and plainLM}{26}{section.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Experimental }{26}{section.5.6}\protected@file@percent }
\bibstyle{alpha}
\bibdata{bibliography}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Discussion and Future Work}{27}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{NGdescent}{Ama98}
\bibcite{bernstein2025deriving}{Ber25}
\bibcite{bernstein2024modulardualitydeeplearning}{BN24a}
\bibcite{bernstein2024oldoptimizernewnorm}{BN24b}
\bibcite{Goodfellow-et-al-2016}{GBC16}
\bibcite{gupta2018shampoopreconditionedstochastictensor}{GKS18}
\bibcite{huang2018denselyconnectedconvolutionalnetworks}{HLvdMW18}
\bibcite{Idelbayev18a}{Ide}
\bibcite{jordan2024muon}{JJB{$^{+}$}24}
\bibcite{Karpathy2022}{Kar22}
\bibcite{kingma2017adammethodstochasticoptimization}{KB17}
\bibcite{loshchilov2019decoupledweightdecayregularization}{LH19}
\bibcite{moonshot}{LSY{$^{+}$}25}
\bibcite{pethick2025trainingdeeplearningmodels}{PXA{$^{+}$}25}
\bibcite{swiGLU}{Sha20}
\bibcite{yuan2020eadamoptimizerepsilonimpact}{YG20}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}How Exponential Moving Averages Smooth the Update Signal}{31}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\gdef \@abspage@last{31}
